{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "- Separate training and testing data set.\n",
    "- Evaluate results on the testing set.\n",
    "- Prefer a simple model over a complicated model that is only marginally better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and underfitting\n",
    "- Type of errors\n",
    "  - Underfitting\n",
    "    - Use a simple model to solve a complicated problem.\n",
    "    - Error due to bias.\n",
    "  - Overfitting\n",
    "    - Use an overly-specific model and fail to generalize.\n",
    "    - Error due to variance.\n",
    "- It is hard to find the right network structure.\n",
    "- Prefer to error on complicated models, and use various techniques to prevent overfitting.\n",
    "\n",
    "### Early stopping\n",
    "- Do gradient descent until the testing error stops decreasing and starts to increase.\n",
    "\n",
    "<img src=\"images/early-stopping.png\">\n",
    "\n",
    "### Regularization\n",
    "- Large coefficients lead to overfitting.\n",
    "\n",
    "<img src=\"images/active-function-too-certain.png\">\n",
    "\n",
    "<img src=\"images/russell-quote.png\">\n",
    "\n",
    "- Penalize large weights by add the weights into the error function.\n",
    "- Original error function\n",
    "$$ E = - \\dfrac{1}{m} \\sum_{i=1}^{m}{(1-y_i)ln(1-\\hat{y}_i)} + y_i ln(\\hat{y}_i) $$\n",
    "- L1 regularization\n",
    "$$ \\lambda(\\sum_{i=1}^{n}{|w_i|})$$\n",
    "  - Add sum of absolute values of the weights.\n",
    "  - Tend to end up with sparse vectors.\n",
    "  - Small weights tend to go to zero.\n",
    "  - Example: $(1, 0, 0, 1, 0)$\n",
    "  - Good for feature selection (reduce the number of weights).\n",
    "\n",
    "- L2 regularization\n",
    "$$ \\lambda(\\sum_{i=1}^{n}{w_i^2})$$\n",
    "  - Add sum of the squared values of the weights.\n",
    "  - Tend to maintain all the weights homogeneously small.\n",
    "  - Example: $(0.5, 0.3, -0.2, -0.4, 0.1)$\n",
    "  - Good for training models.\n",
    "\n",
    "- $\\lambda$ determines how much to penalize the coefficients.\n",
    "- Compare two vectors: $(1, 0)$ and $(0.5, 0.5)$. Both L1 and L2 regulations result in $1$ for the former vector. For the latter vector, L1 leads to $1$, but L2 leads to $0.5$. So L2 prefers the latter over the former.\n",
    "\n",
    "### Dropout\n",
    "- Sometimes one part of the network has very large weights, dominating all the training.\n",
    "- For each epoch, randomly turn of some of the nodes. The other parts of the network have to pick up the slack, and take more part in the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local minima\n",
    "- Gradient descent can stuck at a local minimum.\n",
    "\n",
    "### Random restart\n",
    "- Perform gradient descent from multiple random places.\n",
    "\n",
    "### Momentum\n",
    "- When stuck at a local minimum, move with momentum to get over the hump to look for a lower minimum.\n",
    "- Use the weighted average of previous 3 or 4 gradients.\n",
    "- Momentum\n",
    "  - $\\beta$: a constant in the range $[0, 1]$.\n",
    "  - $step(n) \\rightarrow step(n) + \\beta step(n-1) + \\beta^2 step(n-2) + ...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing gradient\n",
    "- Derivative is the element that tells in what direction to move.\n",
    "- When the gradient is too small, each epoch makes very little progress.\n",
    "  - Sigmoid function gets flat at far left or right.\n",
    "  - Its derivative is small.\n",
    "\n",
    "<img src=\"images/flat-sigmoid-function.png\">\n",
    "\n",
    "- Solution: use other activation function.\n",
    "\n",
    "### Hyperbolic tangent function\n",
    "  - $ tanh(x) = \\dfrac{e^x - e^{-x}}{e^x + e^{-x}} $\n",
    "  - The range is $[-1, 1]$ and has larger derivative.\n",
    "  <img src=\"images/hyperbolic-tangent-function.png\">\n",
    "\n",
    "### Rectified linear unit (ReLU)\n",
    "  - $ relu(x) = x \\geq 0 \\ ? \\ x \\ : \\ 0 $\n",
    "  - Can improve the training significantly without sacrificing much accuracy.\n",
    "  <img src=\"images/relu-function.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch vs stochastic gradient descent\n",
    "- Batch: run through all data points in each epoch.\n",
    "- Stochastic: run only a subset of data points.\n",
    "  - Split the data into several batches.\n",
    "  - In each epoch, only run through one batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate decay\n",
    "- With a high learning rate, each step is large, and may miss the minimal, making the model chaotic.\n",
    "- With a low learning rate, each step is small, and may make the model slow.\n",
    "- If the model is not working, decrease the learning rate.\n",
    "- Rule\n",
    "  - If steep: long steps.\n",
    "  - If plain: small steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
